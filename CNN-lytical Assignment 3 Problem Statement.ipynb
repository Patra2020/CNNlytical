{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN-lytical Assignment-3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# CNN-lytical Assignment-3\n",
        "<center>\n",
        "<img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcS7fZ0PJ4leQi4qtXR5Egv5YILqQqvzVSNtFg&usqp=CAU\">\n",
        "</center>\n",
        "\n",
        "*  In this assignment, we will use CNNs in [PyTorch](https://pytorch.org/docs/stable/index.html) for image classification.\n",
        "\n",
        "* We have been using MNIST by flattening 28$\\times$28 images to 784-sized vectors.\n",
        "\n",
        "* This time, we will classify images from the CIFAR-10 dataset - dimension is 32$\\times$32.\n",
        "\n",
        "* Much of this notebook remains the same as for Assignment 2, just minor changes would be needed and this assignment won't take long.\n",
        "\n",
        "**Feel free to redefine any function signatures below, just make sure the final cell remains the same.**"
      ],
      "metadata": {
        "id": "Xwo8D8V-uPsI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import libraries here\n",
        "PyTorch, NumPy, Matplotlib, ...\n",
        "Even when equipped with PyTorch, NumPy and Matplotlib make your work easier for visualization etc.\n",
        "\n",
        "Note the following method to **initialize the seed** for reproducibility of results, both for NumPy & PyTorch (CPU/CUDA)."
      ],
      "metadata": {
        "id": "QQNvtQCE_j1Q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CnnyxVTxqpZB"
      },
      "outputs": [],
      "source": [
        "def set_seed(seed=42):\n",
        "    '''Sets the seed of the entire notebook so results are the same every time we run.\n",
        "    This is for REPRODUCIBILITY.'''\n",
        "    # np.random.seed(seed)\n",
        "    # torch.manual_seed(seed)\n",
        "    # torch.cuda.manual_seed(seed)\n",
        "    # # When running on the CuDNN backend, two further options must be set\n",
        "    # torch.backends.cudnn.deterministic = True\n",
        "    # torch.backends.cudnn.benchmark = False\n",
        "    # # Set a fixed value for the hash seed\n",
        "    # os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    \n",
        "set_seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load *Dataset*\n",
        "Use the [pickle file](https://drive.google.com/file/d/1_IHICOfAsT7x63VBBuN1WMh4bDqtUThp/view?usp=sharing) shared for this assignment here."
      ],
      "metadata": {
        "id": "D6dAe4V0_3zC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load the data set\n",
        "\n",
        "X = pass\n",
        "y = pass\n",
        "\n",
        "\n",
        "# Split into X_train, y_train, X_test, y_test\n",
        "# you can use stratified splitting from sklearn library\n",
        "\n"
      ],
      "metadata": {
        "id": "umr8-1EI_3ez"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# display a 4x4 grid, \n",
        "# choose 16 images randomly, display the images as well as corresponding labels\n"
      ],
      "metadata": {
        "id": "w4174DiUAUIJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating a Dataset Class\n",
        "In PyTorch, there is existing implementation of batch-splitting. You don't need to do it manually over here. Instead, just define a Dataset class and a Dataloader wrapping it.\n",
        "\n",
        "A dataset class must have 3 functions - ```__init__```, ```__len__```, ```__getitem__```. Their names are pretty self-explanatory. You can read more about this [here](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html).\n",
        "\n",
        "You will have to perform normalization, augmentation on the dataset here itself, have a look at [PyTorch Transforms](https://pytorch.org/vision/stable/transforms.html).\n",
        "\n",
        "**Note -** While initializing the dataset class object, make sure you only pass the numpy arrays for images and labels. So the ```__init__``` function should look like\n",
        "```\n",
        "    def __init__(self, X, y):\n",
        "```"
      ],
      "metadata": {
        "id": "ZjY5oNGRAK1e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define your dataset class\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vifSrimqBGjH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ```nn.Module``` for your model\n",
        "In this segment, define a class for your model, it has to inherit from the ```nn.Module``` class. You must define two functions here - ```__init__``` and ```forward```, again pretty self-explanatory. Helper functions can also be implemented, your choice!\n",
        "\n",
        "Look into the following ```torch``` layers beyond those you used in the second assignment and combine them to form your network, you can find more [here](https://pytorch.org/docs/stable/nn.html) -\n",
        "- [```nn.Conv2d```](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html)\n",
        "- [```nn.BatchNorm2d```](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html)\n"
      ],
      "metadata": {
        "id": "DOs6uifpBF8P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define a child class of nn.Module for your model\n",
        "# specify the architecture here itself\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6Mr6_5pzGRjp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training loop\n",
        "You can write a training loop but usually writing it within a function helps so that you can train in multiple passes with just one function call if you still don't see convergence of the loss. ```display_step``` is for you to display results on the validation set (which you must not have trained upon).\n",
        "\n",
        "You will need to use ```zero_grad()```, ```backward()``` and multiple such functions here. Look for them in the tutorials given."
      ],
      "metadata": {
        "id": "tVTyirdELXlt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, optimizer, criterion, train_loader, display_step=None):\n",
        "    pass"
      ],
      "metadata": {
        "id": "z0BnrNm8LN5s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initialize weights\n",
        "Write a small function to initialize weights for your model. You don't need to do it individually for each layer, there are ways to do it in a simple ```for``` loop."
      ],
      "metadata": {
        "id": "g319ipPXMh0R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def init_weights(...):\n",
        "    pass"
      ],
      "metadata": {
        "id": "GRqqKNLZMjDe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prediction & Accuracy\n",
        "Prediction function should predict outputs using your trained model for a given **NumPy array** ```X_test``` and the output should be another **NumPy array**.\n",
        "\n",
        "The accuracy function would be the same as before."
      ],
      "metadata": {
        "id": "ivuHRGtfN3sE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(model, X_test):\n",
        "    pass"
      ],
      "metadata": {
        "id": "cPX1q_0AN3ZV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(pred, labels):\n",
        "    pass"
      ],
      "metadata": {
        "id": "_nKROVpWOa6j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Actually training your model\n",
        "- Create a model, initialize it. Define optimizer for the model as well as loss criterion (you can actually set the seed here again, just in case you did some ```rand``` calls above for testing your functions).\n",
        "- Define an instance of the dataset class, wrap it in a dataloader.\n",
        "- Call the train function and train your model!\n"
      ],
      "metadata": {
        "id": "8aA1EWZmMbQe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "F8JG_XURNLmr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run your model for the validation dataset\n",
        "Use your trained model to get predictions for the validation dataset you split earlier."
      ],
      "metadata": {
        "id": "OQsiK0-COe6E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "i_B_NUjUOq3c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Submission\n",
        "To submit your solution, you will need to make a file with name ```model.py``` containing imports necessary to write the model class and the model class itself. It shouldn't do anything else when run. Also create a file ```dataset.py``` with the dataset class and all necessary imports. Other than this, save the trained model in a file named ```ass_2.pt```. When you are done with the assignment, commit the updated notebook, the ```model.py```, ```dataset.py``` class files and the ```ass_2.pt``` model-weights file to the repository."
      ],
      "metadata": {
        "id": "0f4W_facj-PA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "torch.save(final_model, 'ass_2.pt')\n",
        "files.download('ass_2.pt') # download the file from the Colab session for submission"
      ],
      "metadata": {
        "id": "7tknYAy1j92M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check if it got saved right!"
      ],
      "metadata": {
        "id": "flMRBW9Akhkg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load the model, use predict function\n"
      ],
      "metadata": {
        "id": "-wA9nHzYkj1R"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
