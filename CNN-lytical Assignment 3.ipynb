{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN_lytical_Assignment_3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# CNN-lytical Assignment-3\n",
        "<center>\n",
        "<img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcS7fZ0PJ4leQi4qtXR5Egv5YILqQqvzVSNtFg&usqp=CAU\">\n",
        "</center>\n",
        "\n",
        "*  In this assignment, we will use CNNs in [PyTorch](https://pytorch.org/docs/stable/index.html) for image classification.\n",
        "\n",
        "* We have been using MNIST by flattening 28$\\times$28 images to 784-sized vectors.\n",
        "\n",
        "* This time, we will classify images from the CIFAR-10 dataset - dimension is 32$\\times$32.\n",
        "\n",
        "* Much of this notebook remains the same as for Assignment 2, just minor changes would be needed and this assignment won't take long.\n",
        "\n",
        "**Feel free to redefine any function signatures below, just make sure the final cell remains the same.**"
      ],
      "metadata": {
        "id": "Xwo8D8V-uPsI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import libraries here\n",
        "PyTorch, NumPy, Matplotlib, ...\n",
        "Even when equipped with PyTorch, NumPy and Matplotlib make your work easier for visualization etc.\n",
        "\n",
        "Note the following method to **initialize the seed** for reproducibility of results, both for NumPy & PyTorch (CPU/CUDA)."
      ],
      "metadata": {
        "id": "QQNvtQCE_j1Q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CnnyxVTxqpZB"
      },
      "outputs": [],
      "source": [
        "def set_seed(seed=42):\n",
        "    '''Sets the seed of the entire notebook so results are the same every time we run.\n",
        "    This is for REPRODUCIBILITY.'''\n",
        "    # np.random.seed(seed)\n",
        "    # torch.manual_seed(seed)\n",
        "    # torch.cuda.manual_seed(seed)\n",
        "    # # When running on the CuDNN backend, two further options must be set\n",
        "    # torch.backends.cudnn.deterministic = True\n",
        "    # torch.backends.cudnn.benchmark = False\n",
        "    # # Set a fixed value for the hash seed\n",
        "    # os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    \n",
        "set_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "bW1IrhQlV0go"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "3asBWDLNVzHF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load *Dataset*\n",
        "Use the [pickle file](https://drive.google.com/file/d/1_IHICOfAsT7x63VBBuN1WMh4bDqtUThp/view?usp=sharing) shared for this assignment here."
      ],
      "metadata": {
        "id": "D6dAe4V0_3zC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load the data set\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import pickle\n",
        "\n",
        "DATA_PATH = \"/content/drive/My Drive\"\n",
        "infile = open(DATA_PATH+'/train_data.pkl','rb')\n",
        "best_model2 = pickle.load(infile)\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "var=pd.read_pickle('/content/drive/My Drive/train_cifar.pkl')\n",
        "Y = var['y']\n",
        "X = var['X']\n",
        "print(X.shape)\n",
        "\n",
        "from sklearn import preprocessing\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "StandDev = np.std(X,axis = 0,keepdims = True)\n",
        "Mean= np.mean(X,axis = 0,keepdims=True)\n",
        "Normalized_X = (X-Mean)/(StandDev + 1e-7)\n",
        "Normalized_X = np.reshape(Normalized_X ,(50000,3,32,32))\n",
        "print(Normalized_X.shape)\n",
        "testsize = 0.5\n",
        "X_train, X_test, y_train, y_test = train_test_split(Normalized_X, Y, test_size = testsize,random_state = 42)\n",
        "\n",
        "# Split into X_train, y_train, X_test, y_test\n",
        "# you can use stratified splitting from sklearn library\n",
        "\n"
      ],
      "metadata": {
        "id": "umr8-1EI_3ez",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2e96aac-b100-4de1-c34b-1ee2f716221c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "(50000, 32, 32, 3)\n",
            "(50000, 3, 32, 32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# display a 4x4 grid, \n",
        "# choose 16 images randomly, display the images as well as corresponding labels\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "for i in range(16):\n",
        "    plt.subplot(4,4,i+1)\n",
        "    index=random.randrange(0,50000)\n",
        "    image = X[index]\n",
        "    image = np.array(image, dtype='float')\n",
        "    #pixels = image.reshape((32,32,3))\n",
        "    #plt.imshow(pixels)\n",
        "\n",
        "\n",
        "    im_r = image[0:1024].reshape(32, 32,1)\n",
        "    im_g = image[1024:2048].reshape(32, 32,1)\n",
        "    im_b = image[2048:].reshape(32, 32,1)\n",
        "\n",
        "    img = np.dstack((im_r, im_g, im_b))\n",
        "    plt.title(Y[index][0])\n",
        "    plt.axis('off')\n",
        "plt.subplots_adjust(left=0.1,\n",
        "                    bottom=0.1, \n",
        "                    right=0.9, \n",
        "                    top=0.9, \n",
        "                    wspace=0.4, \n",
        "                    hspace=0.4)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "w4174DiUAUIJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 318
        },
        "outputId": "98465296-d3a0-45a6-bc4d-00a06d9a86b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-3fb7a883938b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mim_r\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mim_g\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2048\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mim_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2048\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 3072 into shape (32,32,1)"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAHUAAABSCAYAAABqrZsyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAEQUlEQVR4nO2dTWhcVRiGn9fWKmShYLMoWqjFYsjCRTtIVyKI0GaRLnSRbjRSCUWLa8GF0I24EsRiCVr8WdRiVxEUERRcWTsBf1pFSQUxUjCt0o1QDXwu7k0N0yRz7uTcubcf3wMDc+fcn5d5uDOHw3fukZkR+OK2pgME+QmpDgmpDgmpDgmpDgmpDukrVdIpSX9IurBOuyS9LmlB0neS9uaPGVQh5U59BziwQftBYE/5mgHe3HysYDP0lWpmXwJ/brDLIeA9K/gKuFvSjlwBg+rk+E+9F/ht1fZi+VnQEFuHeTFJMxQ/0YyMjOwbGxsb5uVvOebn56+Y2WjV43JI/R3YuWr7vvKzmzCzWWAWoNPpWLfbzXB5v0j6dZDjcvz8zgFPlb3g/cA1M7uc4bzBgPS9UyWdBh4FtktaBF4Gbgcws5PAx8AEsAD8DTxTV9ggjb5Szexwn3YDns+WKNg0MaLkkJDqkJDqkJDqkJDqkJDqkJDqkJDqkJDqkJDqkJDqkJDqkJDqkJDqkJDqkCSpkg5I+qms7X1xjfZpSUuSvilfz+aPGqSSUvmwBTgBPE5RKXhe0pyZ/dCz6xkzO1ZDxqAiKXfqw8CCmf1iZv8AH1DU+gYtJUVqal3vE+W0i7OSdq7RjqQZSV1J3aWlpQHiBink6ih9BOwys4eAz4B319rJzGbNrGNmndHRyuWsQSIpUvvW9ZrZVTO7Xm6+BezLEy8YhBSp54E9ku6XtA2Yoqj1vUHP3JlJ4Md8EYOqpJSILks6BnwKbAFOmdlFSceBrpnNAS9ImgSWKSZTTdeYOeiDmnrkTky76I+keTPrVD0uRpQcElIdElIdElIdElIdElIdElIdElIdElIdElIdElIdElIdElIdElIdkqtE9A5JZ8r2c5J25Q4apJPyvN+VEtGDwDhwWNJ4z25HgL/M7AHgNeDV3EGDdHKViB7i/2Kzs8BjkpQvZlCFXCWiN/Yxs2XgGnBPjoBBdRp7NCxwfb1HuDfEduBK0yF6eHCQg1Kkpjz6dWWfRUlbgbuAq70nWv1oWEndQepv6qJteaDINMhxWUpEy+2ny/dPAp9bLCLXGLlKRN8G3pe0QFEiOlVn6GBjGisRlTRT/hy3grblgcEzNSY1qI8YJnRI7VLbNsTYtlnxtazQZWa1vSg6VpeA3cA24FtgvGef54CT5fspihnpTeaZBt6o83vpud4jwF7gwjrtE8AngID9wLl+56z7Tm3bEGPrZsVbDSt01S21bUOM2WbFD5HKK3RFR+lmkmbFt5m6pVYZYmSjIcZh5WnhrPjkFbpWqFtq24YYb8VZ8dVX6BpC724C+Jmi1/lS+dlxYLJ8fyfwIcUKVF8DuxvO8wpwkaJn/AUwVnOe08Bl4F+K/8sjwFHgaNkuiiKFS8D3QKffOWNEySHRUXJISHVISHVISHVISHVISHVISHVISHXIfxdQlctdwI0AAAAAAElFTkSuQmCC\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating a Dataset Class\n",
        "In PyTorch, there is existing implementation of batch-splitting. You don't need to do it manually over here. Instead, just define a Dataset class and a Dataloader wrapping it.\n",
        "\n",
        "A dataset class must have 3 functions - ```__init__```, ```__len__```, ```__getitem__```. Their names are pretty self-explanatory. You can read more about this [here](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html).\n",
        "\n",
        "You will have to perform normalization, augmentation on the dataset here itself, have a look at [PyTorch Transforms](https://pytorch.org/vision/stable/transforms.html).\n",
        "\n",
        "**Note -** While initializing the dataset class object, make sure you only pass the numpy arrays for images and labels. So the ```__init__``` function should look like\n",
        "```\n",
        "    def __init__(self, X, y):\n",
        "```"
      ],
      "metadata": {
        "id": "ZjY5oNGRAK1e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define your dataset class\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "from torchvision.io import read_image\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class CIFAR_Dataset(Dataset):\n",
        "  def __init__(self, x, y):\n",
        "    self.images = x\n",
        "    self.labels = y\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.labels)\n",
        "  \n",
        "  def __getitem__(self, idx):\n",
        "    return self.images[idx], self.labels[idx]\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_dataset = CIFAR_Dataset(X_train, y_train)\n",
        "test_dataset = CIFAR_Dataset(X_test, y_test)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size = 64, shuffle = True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size =64, shuffle = True)\n",
        "\n"
      ],
      "metadata": {
        "id": "vifSrimqBGjH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ```nn.Module``` for your model\n",
        "In this segment, define a class for your model, it has to inherit from the ```nn.Module``` class. You must define two functions here - ```__init__``` and ```forward```, again pretty self-explanatory. Helper functions can also be implemented, your choice!\n",
        "\n",
        "Look into the following ```torch``` layers beyond those you used in the second assignment and combine them to form your network, you can find more [here](https://pytorch.org/docs/stable/nn.html) -\n",
        "- [```nn.Conv2d```](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html)\n",
        "- [```nn.BatchNorm2d```](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html)\n"
      ],
      "metadata": {
        "id": "DOs6uifpBF8P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define a child class of nn.Module for your model\n",
        "# specify the architecture here itself\n",
        "\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 16 * 5 * 5)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "6Mr6_5pzGRjp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_device():\n",
        "    if torch.cuda.is_available():\n",
        "        device = 'cuda:0'\n",
        "    else:\n",
        "        device = 'cpu'\n",
        "    return device\n",
        "device = get_device()"
      ],
      "metadata": {
        "id": "vjESO-qPefbo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print( torch.cuda.is_available())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lArU-ye4Tn4K",
        "outputId": "9256bc4a-2522-49b2-a527-533f1ea956d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = Net().to(device)\n",
        "print(model)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 0.0001)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wAD9oJn9e3kV",
        "outputId": "746977e1-1d50-4aab-e07e-6e3fe55daf57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Net(\n",
            "  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
            "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
            "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training loop\n",
        "You can write a training loop but usually writing it within a function helps so that you can train in multiple passes with just one function call if you still don't see convergence of the loss. ```display_step``` is for you to display results on the validation set (which you must not have trained upon).\n",
        "\n",
        "You will need to use ```zero_grad()```, ```backward()``` and multiple such functions here. Look for them in the tutorials given."
      ],
      "metadata": {
        "id": "tVTyirdELXlt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def train(model, criterion, optimizer, train_dataloader, epochs):\n",
        "  for epoch in range(epochs):\n",
        "    running_loss = 0\n",
        "    for data in train_dataloader:\n",
        "      inputs, labels = data[0].to(device), data[1].to(device)\n",
        "      optimizer.zero_grad()\n",
        "      outputs = model(inputs.float())\n",
        "      labels = torch.flatten(labels)\n",
        "      loss = criterion(outputs, labels.type(torch.LongTensor))\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      running_loss += loss.item()\n",
        "\n",
        "    print('[Epoch %d] loss: %.3f' % (epoch + 1, running_loss/len(train_dataloader)))\n",
        "  \n",
        "  print(\"training complete\")"
      ],
      "metadata": {
        "id": "z0BnrNm8LN5s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initialize weights\n",
        "Write a small function to initialize weights for your model. You don't need to do it individually for each layer, there are ways to do it in a simple ```for``` loop."
      ],
      "metadata": {
        "id": "g319ipPXMh0R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def init_weights(...):\n",
        "    pass"
      ],
      "metadata": {
        "id": "GRqqKNLZMjDe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prediction & Accuracy\n",
        "Prediction function should predict outputs using your trained model for a given **NumPy array** ```X_test``` and the output should be another **NumPy array**.\n",
        "\n",
        "The accuracy function would be the same as before."
      ],
      "metadata": {
        "id": "ivuHRGtfN3sE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def predict(model, test_dataloader):\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  with torch.no_grad():\n",
        "    for data in test_dataloader:\n",
        "      inputs, labels = data[0].to(device), data[1].to(device)\n",
        "      labels = torch.flatten(labels).type(torch.LongTensor)\n",
        "      outputs = model(inputs.float())\n",
        "      _, predicted = torch.max(outputs.data,1)\n",
        "      total += labels.size(0)\n",
        "      correct += (predicted == labels).sum().item()\n",
        "  print('Accuracy of the network on test images: %0.3f %%' % (100 * correct / total))\n",
        "      #model.eval()"
      ],
      "metadata": {
        "id": "cPX1q_0AN3ZV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(pred, labels):\n",
        "     "
      ],
      "metadata": {
        "id": "_nKROVpWOa6j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Actually training your model\n",
        "- Create a model, initialize it. Define optimizer for the model as well as loss criterion (you can actually set the seed here again, just in case you did some ```rand``` calls above for testing your functions).\n",
        "- Define an instance of the dataset class, wrap it in a dataloader.\n",
        "- Call the train function and train your model!\n"
      ],
      "metadata": {
        "id": "8aA1EWZmMbQe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train(model,criterion,optimizer,train_dataloader,30)"
      ],
      "metadata": {
        "id": "F8JG_XURNLmr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5a26002-c039-4fe2-ed7d-e9d41c61b3f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 1] loss: 1.616\n",
            "[Epoch 2] loss: 1.606\n",
            "[Epoch 3] loss: 1.598\n",
            "[Epoch 4] loss: 1.592\n",
            "[Epoch 5] loss: 1.583\n",
            "[Epoch 6] loss: 1.573\n",
            "[Epoch 7] loss: 1.567\n",
            "[Epoch 8] loss: 1.559\n",
            "[Epoch 9] loss: 1.552\n",
            "[Epoch 10] loss: 1.547\n",
            "[Epoch 11] loss: 1.538\n",
            "[Epoch 12] loss: 1.530\n",
            "[Epoch 13] loss: 1.525\n",
            "[Epoch 14] loss: 1.516\n",
            "[Epoch 15] loss: 1.508\n",
            "[Epoch 16] loss: 1.502\n",
            "[Epoch 17] loss: 1.495\n",
            "[Epoch 18] loss: 1.489\n",
            "[Epoch 19] loss: 1.483\n",
            "[Epoch 20] loss: 1.476\n",
            "[Epoch 21] loss: 1.470\n",
            "[Epoch 22] loss: 1.464\n",
            "[Epoch 23] loss: 1.459\n",
            "[Epoch 24] loss: 1.454\n",
            "[Epoch 25] loss: 1.447\n",
            "[Epoch 26] loss: 1.440\n",
            "[Epoch 27] loss: 1.436\n",
            "[Epoch 28] loss: 1.430\n",
            "[Epoch 29] loss: 1.424\n",
            "[Epoch 30] loss: 1.423\n",
            "training complete\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run your model for the validation dataset\n",
        "Use your trained model to get predictions for the validation dataset you split earlier."
      ],
      "metadata": {
        "id": "OQsiK0-COe6E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "i_B_NUjUOq3c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Submission\n",
        "To submit your solution, you will need to make a file with name ```model.py``` containing imports necessary to write the model class and the model class itself. It shouldn't do anything else when run. Also create a file ```dataset.py``` with the dataset class and all necessary imports. Other than this, save the trained model in a file named ```ass_2.pt```. When you are done with the assignment, commit the updated notebook, the ```model.py```, ```dataset.py``` class files and the ```ass_2.pt``` model-weights file to the repository."
      ],
      "metadata": {
        "id": "0f4W_facj-PA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "torch.save(final_model, 'ass_2.pt')\n",
        "files.download('ass_2.pt') # download the file from the Colab session for submission"
      ],
      "metadata": {
        "id": "7tknYAy1j92M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check if it got saved right!"
      ],
      "metadata": {
        "id": "flMRBW9Akhkg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load the model, use predict function\n",
        "predict(model, test_dataloader)"
      ],
      "metadata": {
        "id": "-wA9nHzYkj1R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6619f2f6-61bd-4e80-9908-433dd5ce6207"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network on test images: 39.720 %\n"
          ]
        }
      ]
    }
  ]
}